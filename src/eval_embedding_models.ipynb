{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from openai import OpenAI\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = '../data/arxiv_metadata_sample.parquet.gzip'\n",
    "# PARQUET_PATH = r\"C:\\Users\\ihett\\OneDrive\\Gabrilyi\\arxiv_project\\arxiv_metadata_sample.parquet.gzip\"\n",
    "\n",
    "EVAL_DF_PATH = '../data/eval_df.parquet.gzip'\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data\"\n",
    "# CHROMA_DATA_PATH = r\"C:\\Users\\ihett\\OneDrive\\Gabrilyi\\arxiv_project\\chroma_data\"\n",
    "\n",
    "# [WARNING]\n",
    "# Choose whether to delete all chroma data for the chosen model and recompute it\n",
    "#\n",
    "DO_DELETE_CHROMA_DATA = True\n",
    "\n",
    "#\n",
    "# Choose model style [sentence_transformers, lmstudio]\n",
    "#\n",
    "model_style = \"sentence_transformers\"\n",
    "\n",
    "\n",
    "#\n",
    "# Models from LMStudio\n",
    "#\n",
    "# EMBED_MODEL = \"gte-small-gguf\" # LMStudio (ChristianAzinn/gte-small-gguf/gte-small.Q4_0.gguf)\n",
    "\n",
    "\n",
    "#\n",
    "# Models from Sentence Transformers (https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)\n",
    "#\n",
    "# EMBED_MODEL = \"all-MiniLM-L12-v2\"\n",
    "# EMBED_MODEL = \"all-mpnet-base-v2\"\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_publications.py\n",
    "EMBED_MODEL = \"allenai-specter\" # https://huggingface.co/sentence-transformers/allenai-specter\n",
    "# EMBED_MODEL = \"GIST-small-Embedding-v0\" # https://huggingface.co/avsolatorio/GIST-small-Embedding-v0\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"arxiv_papers\"\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "CHROMA_DATA_PATH = os.path.join(CHROMA_DATA_PATH, EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159136, 13)\n",
      "(3000, 8)\n",
      "Columns in data_df: Index(['id', 'title', 'abstract', 'categories', 'update_date', 'title_words',\n",
      "       'abstract_words', 'mapped_categories', 'amount_categories',\n",
      "       'update_year', 'super_categories', 'super_category',\n",
      "       'amount_super_categories', 'rewritten_text', 'removed_text_25',\n",
      "       'removed_text_50', 'removed_text_75', 'removed_text_25_shuffled',\n",
      "       'removed_text_50_shuffled', 'removed_text_75_shuffled'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "arxiv_df = pd.read_parquet(PARQUET_PATH)\n",
    "eval_df = pd.read_parquet(EVAL_DF_PATH)\n",
    "\n",
    "print(arxiv_df.shape)\n",
    "print(eval_df.shape)\n",
    "\n",
    "# only keep arxiv papers that are in the evaluation set\n",
    "data_df = arxiv_df[arxiv_df['id'].isin(eval_df['id'])]\n",
    "data_df = data_df.merge(eval_df, on='id', how='inner')\n",
    "\n",
    "print(f'Columns in data_df: {data_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>title_words</th>\n",
       "      <th>abstract_words</th>\n",
       "      <th>mapped_categories</th>\n",
       "      <th>amount_categories</th>\n",
       "      <th>update_year</th>\n",
       "      <th>...</th>\n",
       "      <th>super_category</th>\n",
       "      <th>amount_super_categories</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>removed_text_25</th>\n",
       "      <th>removed_text_50</th>\n",
       "      <th>removed_text_75</th>\n",
       "      <th>removed_text_25_shuffled</th>\n",
       "      <th>removed_text_50_shuffled</th>\n",
       "      <th>removed_text_75_shuffled</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs/0703062</td>\n",
       "      <td>Bandit Algorithms for Tree Search</td>\n",
       "      <td>Bandit based methods for tree search have re...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>[Machine Learning]</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Optimizing Tree Search Algorithms for Efficie...</td>\n",
       "      <td>Bandit based methods for tree search have gain...</td>\n",
       "      <td>Bandit based tree have recently to e.g. game g...</td>\n",
       "      <td>for recently to game (Gelly al., 2006). (Kocsi...</td>\n",
       "      <td>rewards effective O(exp(exp(D))) algorithms se...</td>\n",
       "      <td>UpperConfidence to cases, a We sub-optimal Smo...</td>\n",
       "      <td>of possible scalesexponentially performed al.,...</td>\n",
       "      <td>Bandit Algorithms for Tree Search [SEP] Bandit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1410.7743</td>\n",
       "      <td>Data Driven Authentication: On the Effectivene...</td>\n",
       "      <td>We propose a lightweight, and temporally and...</td>\n",
       "      <td>[cs.CR]</td>\n",
       "      <td>2014-10-29</td>\n",
       "      <td>14</td>\n",
       "      <td>128</td>\n",
       "      <td>[Cryptography and Security]</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Behavioral Biometric Authentication Using Mac...</td>\n",
       "      <td>We propose lightweight, temporally and spatial...</td>\n",
       "      <td>propose a temporally user technique sensor-bas...</td>\n",
       "      <td>propose and aware user for the sufficiently fr...</td>\n",
       "      <td>data sufficiently thebackground, We norm,actio...</td>\n",
       "      <td>the model switches propose norm,actions capabi...</td>\n",
       "      <td>duration investigate drift. expected propose o...</td>\n",
       "      <td>Data Driven Authentication: On the Effectivene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2306.13023</td>\n",
       "      <td>AugDMC: Data Augmentation Guided Deep Multiple...</td>\n",
       "      <td>Clustering aims to group similar objects tog...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>7</td>\n",
       "      <td>218</td>\n",
       "      <td>[Computer Vision and Pattern Recognition]</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Discovering multiple perspectives in complex ...</td>\n",
       "      <td>Clustering aims to group similar objects toget...</td>\n",
       "      <td>Clustering aims group objects together separat...</td>\n",
       "      <td>aims while apart. an manner. provide only a cl...</td>\n",
       "      <td>dataaugmentations can the aims prototype-based...</td>\n",
       "      <td>prototype-based methods methods aspects augmen...</td>\n",
       "      <td>method. different as independent aspect in fro...</td>\n",
       "      <td>AugDMC: Data Augmentation Guided Deep Multiple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  cs/0703062                  Bandit Algorithms for Tree Search   \n",
       "1   1410.7743  Data Driven Authentication: On the Effectivene...   \n",
       "2  2306.13023  AugDMC: Data Augmentation Guided Deep Multiple...   \n",
       "\n",
       "                                            abstract categories update_date  \\\n",
       "0    Bandit based methods for tree search have re...    [cs.LG]  2016-08-14   \n",
       "1    We propose a lightweight, and temporally and...    [cs.CR]  2014-10-29   \n",
       "2    Clustering aims to group similar objects tog...    [cs.CV]  2023-06-23   \n",
       "\n",
       "   title_words  abstract_words                          mapped_categories  \\\n",
       "0            5             223                         [Machine Learning]   \n",
       "1           14             128                [Cryptography and Security]   \n",
       "2            7             218  [Computer Vision and Pattern Recognition]   \n",
       "\n",
       "   amount_categories  update_year  ...    super_category  \\\n",
       "0                  1         2016  ...  Computer Science   \n",
       "1                  1         2014  ...  Computer Science   \n",
       "2                  1         2023  ...  Computer Science   \n",
       "\n",
       "  amount_super_categories                                     rewritten_text  \\\n",
       "0                       1  \"Optimizing Tree Search Algorithms for Efficie...   \n",
       "1                       1  \"Behavioral Biometric Authentication Using Mac...   \n",
       "2                       1  \"Discovering multiple perspectives in complex ...   \n",
       "\n",
       "                                     removed_text_25  \\\n",
       "0  Bandit based methods for tree search have gain...   \n",
       "1  We propose lightweight, temporally and spatial...   \n",
       "2  Clustering aims to group similar objects toget...   \n",
       "\n",
       "                                     removed_text_50  \\\n",
       "0  Bandit based tree have recently to e.g. game g...   \n",
       "1  propose a temporally user technique sensor-bas...   \n",
       "2  Clustering aims group objects together separat...   \n",
       "\n",
       "                                     removed_text_75  \\\n",
       "0  for recently to game (Gelly al., 2006). (Kocsi...   \n",
       "1  propose and aware user for the sufficiently fr...   \n",
       "2  aims while apart. an manner. provide only a cl...   \n",
       "\n",
       "                            removed_text_25_shuffled  \\\n",
       "0  rewards effective O(exp(exp(D))) algorithms se...   \n",
       "1  data sufficiently thebackground, We norm,actio...   \n",
       "2  dataaugmentations can the aims prototype-based...   \n",
       "\n",
       "                            removed_text_50_shuffled  \\\n",
       "0  UpperConfidence to cases, a We sub-optimal Smo...   \n",
       "1  the model switches propose norm,actions capabi...   \n",
       "2  prototype-based methods methods aspects augmen...   \n",
       "\n",
       "                            removed_text_75_shuffled  \\\n",
       "0  of possible scalesexponentially performed al.,...   \n",
       "1  duration investigate drift. expected propose o...   \n",
       "2  method. different as independent aspect in fro...   \n",
       "\n",
       "                                                text  \n",
       "0  Bandit Algorithms for Tree Search [SEP] Bandit...  \n",
       "1  Data Driven Authentication: On the Effectivene...  \n",
       "2  AugDMC: Data Augmentation Guided Deep Multiple...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_processing(sample):\n",
    "    title = sample['title']\n",
    "    abstract = sample['abstract']\n",
    "\n",
    "    # remove special characters\n",
    "    title = title.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "    abstract = abstract.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "\n",
    "    # remove multiple spaces\n",
    "    title = ' '.join(title.split())\n",
    "    abstract = ' '.join(abstract.split())\n",
    "\n",
    "    return f\"{title} [SEP] {abstract}\".replace('  ', ' ')\n",
    "\n",
    "data_df['text'] = data_df.apply(text_processing, axis=1)\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadatas(arxiv_df):\n",
    "    metadatas = []\n",
    "    for _, row in arxiv_df.iterrows():\n",
    "        metadatas.append({\n",
    "            \"update_date\": row['update_date'],\n",
    "            \"title_words\": row['title_words'],\n",
    "            \"abstract_words\": row['abstract_words'],\n",
    "            \"super_category\": row['super_category'],\n",
    "            \"mapped_categories\": \";\".join(row['mapped_categories']),\n",
    "        })\n",
    "\n",
    "    return metadatas\n",
    "\n",
    "def create_collection(client, collection_name, embedding_function):\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        get_or_create=True,\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "def delete_collection_data(client, collection, collection_name):\n",
    "    print(f\"Deleting data from collection {collection_name} with {collection.count()} documents\")\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "def get_random_samples_from_collection(collection, n_samples):\n",
    "    collection_ids = collection.get()[\"ids\"]\n",
    "    random_ids = np.random.choice(collection_ids, n_samples, replace=False).tolist()\n",
    "    documents = collection.get(ids=random_ids)\n",
    "    return documents\n",
    "\n",
    "def upsert_data(collection, arxiv_df, metadatas, batch_size):\n",
    "    for i in tqdm(range(0, len(arxiv_df), batch_size)):\n",
    "        collection.upsert(\n",
    "            documents=arxiv_df['text'].iloc[i:i + batch_size].tolist(),\n",
    "            ids=arxiv_df['id'].iloc[i:i + batch_size].tolist(),\n",
    "            metadatas=metadatas[i:i + batch_size],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Workspaces\\Python\\Studium\\Master\\ArxivAbstractProject\\.venv311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "w:\\Workspaces\\Python\\Studium\\Master\\ArxivAbstractProject\\.venv311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if model_style == \"sentence_transformers\":\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBED_MODEL,\n",
    "        device=\"cuda\",\n",
    "        cache_folder=cache_dir\n",
    "    )\n",
    "elif model_style == \"lmstudio\":\n",
    "    class Embedder(EmbeddingFunction):\n",
    "        def __init__(self):\n",
    "            self.client = OpenAI(base_url=\"http://localhost:5000/v1\", api_key=\"lm-studio\")\n",
    "            self.model = EMBED_MODEL\n",
    "\n",
    "        def __call__(self, input:Documents) -> Embeddings:\n",
    "            return [d.embedding for d in self.client.embeddings.create(input = input, model=self.model).data]\n",
    "\n",
    "    embedding_func = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data from collection arxiv_papers with 20000 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993bd9d175c9433c96b7068475e1657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete the collection if it exists\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "########################################\n",
    "######## WARNING: DELETES DATA #########\n",
    "########################################\n",
    "if DO_DELETE_CHROMA_DATA and input(\"Do you want to delete all data in the collection? (y/n): \") == \"y\":\n",
    "    ##### delete if you want to start fresh but then you need to create the collection again\n",
    "    delete_collection_data(client, collection, COLLECTION_NAME)\n",
    "    collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "    ##### create metadatas\n",
    "    metadatas = create_metadatas(data_df)\n",
    "\n",
    "    ##### upsert data (insert or update if exists)\n",
    "    upsert_data(collection, data_df, metadatas, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 1605.08039\n",
      "Sample LLM Text: \"Dark Matter searches via Higgs boson decay at LHC\"\n",
      "Sample Text: Boosting invisible searches via $\\boldsymbol{ZH}$: From the Higgs Boson to Dark Matter Simplified Models [SEP] Higgs boson production in association with a $Z$-boson at the LHC is analysed, both in the Standard Model and in Simplified Model extensions for Dark Matter. We focus on $H\\rightarrow$invisibles searches and show that loop-induced components for both the signal and background present phenomenologically relevant contributions to the $\\mathcal{BR}(H\\rightarrow\\textit{inv})$ limits. In addition, the constraining power of this channel to Simplified Models for Dark Matter with scalar and pseudo-scalar mediators $\\phi$ and $A$ is discussed and compared with non-collider constraints. We find that with $100~fb^{-1}$ of LHC data, this channel provides competitive constraints to the non-collider bounds, for most of the parameter space we consider, bounding the universal Standard Model fermion-mediator strength at $g_v < 1$ for moderate masses in the range of ${100~\\text{GeV}<m_{\\phi/A}<400}$ GeV.\n",
      "#####   ID: 1605.08039   #####\n",
      "#####   ID: 1306.5878   #####\n",
      "#####   ID: hep-ph/0511034   #####\n",
      "#####   ID: 1601.00386   #####\n",
      "#####   ID: 2111.06769   #####\n"
     ]
    }
   ],
   "source": [
    "sample_data = data_df.sample(1)\n",
    "sample_id = sample_data['id'].values[0]\n",
    "sample_llm_text = sample_data['rewritten_text'].values[0]\n",
    "\n",
    "print(f\"Sample ID: {sample_id}\")\n",
    "print(f\"Sample LLM Text: {sample_llm_text}\")\n",
    "print(f\"Sample Text: {sample_data['text'].values[0]}\")\n",
    "\n",
    "top_n_papers = 5\n",
    "query_results = collection.query(query_texts=[sample_llm_text], n_results=top_n_papers)\n",
    "\n",
    "for _id, _doc, _dist, _meta in zip(query_results[\"ids\"][0], query_results[\"documents\"][0], query_results[\"distances\"][0], query_results[\"metadatas\"][0]):\n",
    "    print(f\"#####   ID: {_id}   #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a068f69a683f4aba98d37accad0c9bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n_papers = 20\n",
    "matches = []\n",
    "for idx, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "    paper_id = row['id']\n",
    "    paper_llm_text = row['rewritten_text']\n",
    "    query_results = collection.query(query_texts=[paper_llm_text], n_results=top_n_papers)\n",
    "\n",
    "    found_n = -1\n",
    "    for i, result_id in enumerate(query_results[\"ids\"][0], 1):\n",
    "        if result_id == paper_id:\n",
    "            found_n = i\n",
    "            break\n",
    "\n",
    "    matches.append((paper_id, found_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches, columns=['id', 'found_n'])\n",
    "matches_df['found_n'] = matches_df['found_n'].replace(-1, np.nan)\n",
    "matches_df['found_n'] = matches_df['found_n'].astype(float)\n",
    "data_df = data_df.merge(matches_df, on='id', how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>title_words</th>\n",
       "      <th>abstract_words</th>\n",
       "      <th>mapped_categories</th>\n",
       "      <th>amount_categories</th>\n",
       "      <th>update_year</th>\n",
       "      <th>...</th>\n",
       "      <th>amount_super_categories</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>removed_text_25</th>\n",
       "      <th>removed_text_50</th>\n",
       "      <th>removed_text_75</th>\n",
       "      <th>removed_text_25_shuffled</th>\n",
       "      <th>removed_text_50_shuffled</th>\n",
       "      <th>removed_text_75_shuffled</th>\n",
       "      <th>text</th>\n",
       "      <th>found_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs/0703062</td>\n",
       "      <td>Bandit Algorithms for Tree Search</td>\n",
       "      <td>Bandit based methods for tree search have re...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>[Machine Learning]</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Optimizing Tree Search Algorithms for Efficie...</td>\n",
       "      <td>Bandit based methods for tree search have gain...</td>\n",
       "      <td>Bandit based tree have recently to e.g. game g...</td>\n",
       "      <td>for recently to game (Gelly al., 2006). (Kocsi...</td>\n",
       "      <td>rewards effective O(exp(exp(D))) algorithms se...</td>\n",
       "      <td>UpperConfidence to cases, a We sub-optimal Smo...</td>\n",
       "      <td>of possible scalesexponentially performed al.,...</td>\n",
       "      <td>Bandit Algorithms for Tree Search [SEP] Bandit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1410.7743</td>\n",
       "      <td>Data Driven Authentication: On the Effectivene...</td>\n",
       "      <td>We propose a lightweight, and temporally and...</td>\n",
       "      <td>[cs.CR]</td>\n",
       "      <td>2014-10-29</td>\n",
       "      <td>14</td>\n",
       "      <td>128</td>\n",
       "      <td>[Cryptography and Security]</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Behavioral Biometric Authentication Using Mac...</td>\n",
       "      <td>We propose lightweight, temporally and spatial...</td>\n",
       "      <td>propose a temporally user technique sensor-bas...</td>\n",
       "      <td>propose and aware user for the sufficiently fr...</td>\n",
       "      <td>data sufficiently thebackground, We norm,actio...</td>\n",
       "      <td>the model switches propose norm,actions capabi...</td>\n",
       "      <td>duration investigate drift. expected propose o...</td>\n",
       "      <td>Data Driven Authentication: On the Effectivene...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2306.13023</td>\n",
       "      <td>AugDMC: Data Augmentation Guided Deep Multiple...</td>\n",
       "      <td>Clustering aims to group similar objects tog...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>7</td>\n",
       "      <td>218</td>\n",
       "      <td>[Computer Vision and Pattern Recognition]</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Discovering multiple perspectives in complex ...</td>\n",
       "      <td>Clustering aims to group similar objects toget...</td>\n",
       "      <td>Clustering aims group objects together separat...</td>\n",
       "      <td>aims while apart. an manner. provide only a cl...</td>\n",
       "      <td>dataaugmentations can the aims prototype-based...</td>\n",
       "      <td>prototype-based methods methods aspects augmen...</td>\n",
       "      <td>method. different as independent aspect in fro...</td>\n",
       "      <td>AugDMC: Data Augmentation Guided Deep Multiple...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2209.15157</td>\n",
       "      <td>Rethinking and Recomputing the Value of ML Models</td>\n",
       "      <td>In this paper, we argue that the way we have...</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>8</td>\n",
       "      <td>144</td>\n",
       "      <td>[Machine Learning, Artificial Intelligence]</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Contextualizing Machine Learning for Organiza...</td>\n",
       "      <td>this argue that the way have been training and...</td>\n",
       "      <td>this argue way we training MLmodels largely fo...</td>\n",
       "      <td>the evaluating fact that applied in an as valu...</td>\n",
       "      <td>are some role learning an show learn. societal...</td>\n",
       "      <td>andprovide practices are MLmodels MLmodels we ...</td>\n",
       "      <td>a models models change different different on ...</td>\n",
       "      <td>Rethinking and Recomputing the Value of ML Mod...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2403.19969</td>\n",
       "      <td>Separate, Dynamic and Differentiable (SMART) P...</td>\n",
       "      <td>Deep Neural Network (DNN) pruning has emerge...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>14</td>\n",
       "      <td>161</td>\n",
       "      <td>[Computer Vision and Pattern Recognition, Mach...</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Optimizing Neural Network Pruning for Efficie...</td>\n",
       "      <td>Neural Network (DNN) pruning has as a key stra...</td>\n",
       "      <td>Deep (DNN) has emerged a to improve inference ...</td>\n",
       "      <td>Deep has emerged latency, power techniques, ou...</td>\n",
       "      <td>across pruning demonstrating (SMART) pruner.Th...</td>\n",
       "      <td>results, Deep consumption to accelerating task...</td>\n",
       "      <td>a output forweight parameter tasks various our...</td>\n",
       "      <td>Separate, Dynamic and Differentiable (SMART) P...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  cs/0703062                  Bandit Algorithms for Tree Search   \n",
       "1   1410.7743  Data Driven Authentication: On the Effectivene...   \n",
       "2  2306.13023  AugDMC: Data Augmentation Guided Deep Multiple...   \n",
       "3  2209.15157  Rethinking and Recomputing the Value of ML Models   \n",
       "4  2403.19969  Separate, Dynamic and Differentiable (SMART) P...   \n",
       "\n",
       "                                            abstract      categories  \\\n",
       "0    Bandit based methods for tree search have re...         [cs.LG]   \n",
       "1    We propose a lightweight, and temporally and...         [cs.CR]   \n",
       "2    Clustering aims to group similar objects tog...         [cs.CV]   \n",
       "3    In this paper, we argue that the way we have...  [cs.LG, cs.AI]   \n",
       "4    Deep Neural Network (DNN) pruning has emerge...  [cs.CV, cs.LG]   \n",
       "\n",
       "  update_date  title_words  abstract_words  \\\n",
       "0  2016-08-14            5             223   \n",
       "1  2014-10-29           14             128   \n",
       "2  2023-06-23            7             218   \n",
       "3  2022-10-03            8             144   \n",
       "4  2024-04-01           14             161   \n",
       "\n",
       "                                   mapped_categories  amount_categories  \\\n",
       "0                                 [Machine Learning]                  1   \n",
       "1                        [Cryptography and Security]                  1   \n",
       "2          [Computer Vision and Pattern Recognition]                  1   \n",
       "3        [Machine Learning, Artificial Intelligence]                  2   \n",
       "4  [Computer Vision and Pattern Recognition, Mach...                  2   \n",
       "\n",
       "   update_year  ... amount_super_categories  \\\n",
       "0         2016  ...                       1   \n",
       "1         2014  ...                       1   \n",
       "2         2023  ...                       1   \n",
       "3         2022  ...                       1   \n",
       "4         2024  ...                       1   \n",
       "\n",
       "                                      rewritten_text  \\\n",
       "0  \"Optimizing Tree Search Algorithms for Efficie...   \n",
       "1  \"Behavioral Biometric Authentication Using Mac...   \n",
       "2  \"Discovering multiple perspectives in complex ...   \n",
       "3  \"Contextualizing Machine Learning for Organiza...   \n",
       "4  \"Optimizing Neural Network Pruning for Efficie...   \n",
       "\n",
       "                                     removed_text_25  \\\n",
       "0  Bandit based methods for tree search have gain...   \n",
       "1  We propose lightweight, temporally and spatial...   \n",
       "2  Clustering aims to group similar objects toget...   \n",
       "3  this argue that the way have been training and...   \n",
       "4  Neural Network (DNN) pruning has as a key stra...   \n",
       "\n",
       "                                     removed_text_50  \\\n",
       "0  Bandit based tree have recently to e.g. game g...   \n",
       "1  propose a temporally user technique sensor-bas...   \n",
       "2  Clustering aims group objects together separat...   \n",
       "3  this argue way we training MLmodels largely fo...   \n",
       "4  Deep (DNN) has emerged a to improve inference ...   \n",
       "\n",
       "                                     removed_text_75  \\\n",
       "0  for recently to game (Gelly al., 2006). (Kocsi...   \n",
       "1  propose and aware user for the sufficiently fr...   \n",
       "2  aims while apart. an manner. provide only a cl...   \n",
       "3  the evaluating fact that applied in an as valu...   \n",
       "4  Deep has emerged latency, power techniques, ou...   \n",
       "\n",
       "                            removed_text_25_shuffled  \\\n",
       "0  rewards effective O(exp(exp(D))) algorithms se...   \n",
       "1  data sufficiently thebackground, We norm,actio...   \n",
       "2  dataaugmentations can the aims prototype-based...   \n",
       "3  are some role learning an show learn. societal...   \n",
       "4  across pruning demonstrating (SMART) pruner.Th...   \n",
       "\n",
       "                            removed_text_50_shuffled  \\\n",
       "0  UpperConfidence to cases, a We sub-optimal Smo...   \n",
       "1  the model switches propose norm,actions capabi...   \n",
       "2  prototype-based methods methods aspects augmen...   \n",
       "3  andprovide practices are MLmodels MLmodels we ...   \n",
       "4  results, Deep consumption to accelerating task...   \n",
       "\n",
       "                            removed_text_75_shuffled  \\\n",
       "0  of possible scalesexponentially performed al.,...   \n",
       "1  duration investigate drift. expected propose o...   \n",
       "2  method. different as independent aspect in fro...   \n",
       "3  a models models change different different on ...   \n",
       "4  a output forweight parameter tasks various our...   \n",
       "\n",
       "                                                text found_n  \n",
       "0  Bandit Algorithms for Tree Search [SEP] Bandit...     1.0  \n",
       "1  Data Driven Authentication: On the Effectivene...    19.0  \n",
       "2  AugDMC: Data Augmentation Guided Deep Multiple...     NaN  \n",
       "3  Rethinking and Recomputing the Value of ML Mod...     1.0  \n",
       "4  Separate, Dynamic and Differentiable (SMART) P...     2.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 1406.3183\n",
      "Sample LLM Text: \"Nonlinear Gaussian model sampling with approximate Gaussian flows and importance weights for efficient Bayesian inference.\"\n",
      "Sample Text: Approximations of the Optimal Importance Density using Gaussian Particle Flow Importance Sampling [SEP] Recently developed particle flow algorithms provide an alternative to importance sampling for drawing particles from a posterior distribution, and a number of particle filters based on this principle have been proposed. Samples are drawn from the prior and then moved according to some dynamics over an interval of pseudo-time such that their final values are distributed according to the desired posterior. In practice, implementing a particle flow sampler requires multiple layers of approximation, with the result that the final samples do not in general have the correct posterior distribution. In this paper we consider using an approximate Gaussian flow for sampling with a class of nonlinear Gaussian models. We use the particle flow within an importance sampler, correcting for the discrepancy between the target and actual densities with importance weights. We present a suitable numerical integration procedure for use with this flow and an accompanying step-size control algorithm. In a filtering context, we use the particle flow to sample from the optimal importance density, rather than the filtering density itself, avoiding the need to make analytical or numerical approximations of the predictive density. Simulations using particle flow importance sampling within a particle filter demonstrate significant improvement over standard approximations of the optimal importance density, and the algorithm falls within the standard sequential Monte Carlo framework.\n"
     ]
    }
   ],
   "source": [
    "# show text where the model did not find the paper in the top 20\n",
    "sample_not_found = data_df[data_df['found_n'].isna()].sample(1)\n",
    "sample_not_found_id = sample_not_found['id'].values[0]\n",
    "sample_not_found_llm_text = sample_not_found['rewritten_text'].values[0]\n",
    "sample_not_found_text = sample_not_found['text'].values[0]\n",
    "\n",
    "print(f\"Sample ID: {sample_not_found_id}\")\n",
    "print(f\"Sample LLM Text: {sample_not_found_llm_text}\")\n",
    "print(f\"Sample Text: {sample_not_found_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy: 67.40%\n",
      "Top 3 Accuracy: 80.77%\n",
      "Top 5 Accuracy: 84.30%\n",
      "Top 20 Accuracy: 91.03%\n"
     ]
    }
   ],
   "source": [
    "top_1_accuracy = data_df[data_df['found_n'] == 1].shape[0] / data_df.shape[0] * 100\n",
    "top_3_accuracy = data_df[data_df['found_n'] <= 3].shape[0] / data_df.shape[0] * 100\n",
    "top_5_accuracy = data_df[data_df['found_n'] <= 5].shape[0] / data_df.shape[0] * 100\n",
    "top_20_accuracy = data_df[data_df['found_n'] <= 20].shape[0] / data_df.shape[0] * 100\n",
    "\n",
    "print(f\"Top 1 Accuracy: {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top 3 Accuracy: {top_3_accuracy:.2f}%\")\n",
    "print(f\"Top 5 Accuracy: {top_5_accuracy:.2f}%\")\n",
    "print(f\"Top 20 Accuracy: {top_20_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
