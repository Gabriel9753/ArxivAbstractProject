{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.trychroma.com/guides\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "# EMBED_MODEL = \"all-mpnet-base-v2\"\n",
    "COLLECTION_NAME = \"arxiv_papers\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "CHROMA_DATA_PATH = os.path.join(CHROMA_DATA_PATH, EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "parquet_path = '../data/arxiv_metadata_sample.parquet.gzip'\n",
    "arxiv_df = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(arxiv_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(sample):\n",
    "    title = sample['title']\n",
    "    abstract = sample['abstract']\n",
    "\n",
    "    # remove special characters\n",
    "    title = title.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    abstract = abstract.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "    # remove multiple spaces\n",
    "    title = ' '.join(title.split())\n",
    "    abstract = ' '.join(abstract.split())\n",
    "\n",
    "    return f\"Title: {title} - Abstract: {abstract}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df['text'] = arxiv_df.apply(text_processing, axis=1)\n",
    "arxiv_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadatas(arxiv_df):\n",
    "    metadatas = []\n",
    "    for _, row in arxiv_df.iterrows():\n",
    "        metadatas.append({\n",
    "            \"update_date\": row['update_date'],\n",
    "            \"title_words\": row['title_words'],\n",
    "            \"abstract_words\": row['abstract_words']\n",
    "        })\n",
    "        categories = {f\"category_{i}\": category for i, category in enumerate(row['mapped_categories'])}\n",
    "        metadatas[-1].update(categories)\n",
    "\n",
    "    return metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(client, collection_name, embedding_function):\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        get_or_create=True,\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "def delete_collection_data(client, collection, collection_name):\n",
    "    print(f\"Deleting data from collection {collection_name} with {collection.count()} documents\")\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "def get_random_samples_from_collection(collection, n_samples):\n",
    "    collection_ids = collection.get()[\"ids\"]\n",
    "    random_ids = np.random.choice(collection_ids, n_samples, replace=False).tolist()\n",
    "    documents = collection.get(ids=random_ids)\n",
    "    return documents\n",
    "\n",
    "def upsert_data(collection, arxiv_df, metadatas, batch_size):\n",
    "    for i in tqdm(range(0, len(arxiv_df), batch_size)):\n",
    "        collection.upsert(\n",
    "            documents=arxiv_df['text'].iloc[i:i + batch_size].tolist(),\n",
    "            ids=arxiv_df['id'].iloc[i:i + batch_size].tolist(),\n",
    "            metadatas=metadatas[i:i + batch_size],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the collection if it exists\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "# delete if you want to start fresh but then you need to create the collection again\n",
    "# delete_collection_data(client, collection, COLLECTION_NAME)\n",
    "# collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "# create metadatas\n",
    "metadatas = create_metadatas(arxiv_df)\n",
    "\n",
    "# upsert data (insert or update if exists)\n",
    "# upsert_data(collection, arxiv_df, metadatas, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "words_per_line = 10\n",
    "# define papers to show\n",
    "top_n_papers = 3\n",
    "query = \"The meaning of life in philosophy and the analysis of deep learning language\"\n",
    "print(\"Query:\\n\", query, \"\\n\")\n",
    "query_results = collection.query(query_texts=[query], n_results=top_n_papers)\n",
    "for _id, _doc, _dist, _meta in zip(query_results[\"ids\"][0], query_results[\"documents\"][0], query_results[\"distances\"][0], query_results[\"metadatas\"][0]):\n",
    "    print(f\"#####   ID: {_id}   #####\")\n",
    "    print(f\"Distance: {_dist}\")\n",
    "    print(f\"Metadata: {_meta}\")\n",
    "    _doc_lines = _doc.split()\n",
    "    for i in range(0, len(_doc_lines), words_per_line):\n",
    "        print(\" \".join(_doc_lines[i:i + words_per_line]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = collection.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df = pd.DataFrame({\n",
    "    \"id\": response[\"ids\"],\n",
    "    \"document\": response[\"documents\"],\n",
    "    \"embedding\": response[\"embeddings\"],\n",
    "    \"categories\": [m[\"category_0\"] for m in response[\"metadatas\"]],\n",
    "})\n",
    "all_categories = df[\"categories\"].explode().unique()\n",
    "cat_mapping = {cat: i for i, cat in enumerate(all_categories)}\n",
    "df[\"cat_id\"] = df[\"categories\"].apply(lambda x: cat_mapping[x])\n",
    "df.loc[:, all_categories] = df[\"categories\"].apply(lambda x: [1 if cat in x else 0 for cat in all_categories]).tolist()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from renumics import spotlight\n",
    "\n",
    "# spotlight.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df, cat_ids):\n",
    "        self.ids = df[\"id\"].values\n",
    "        self.embeddings = torch.tensor(df[\"embedding\"].tolist(), dtype=torch.float32)\n",
    "        self.categories = torch.tensor(df[\"cat_id\"].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.embeddings[idx], self.categories[idx]\n",
    "\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, n_emb_size, n_categories):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(n_emb_size, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, n_categories)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# train val split\n",
    "emb_dataset = EmbeddingDataset(df, cat_mapping)\n",
    "train_size = int(0.8 * len(emb_dataset))\n",
    "val_size = len(emb_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(emb_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmbeddingModel(len(df[\"embedding\"].values[0]), len(all_categories)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for _, emb, cat in train_loader:\n",
    "            emb, cat = emb.to(device), cat.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(emb)\n",
    "            loss = criterion(output, cat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for _, emb, cat in val_loader:\n",
    "                emb, cat = emb.to(device), cat.to(device)\n",
    "                output = model(emb)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += cat.size(0)\n",
    "                correct += (predicted == cat).sum().item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {100 * correct / total}\")\n",
    "\n",
    "train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "classes = arxiv_df['mapped_categories'].explode().unique()\n",
    "classes = {c: i for i, c in enumerate(classes)}\n",
    "num_classes = len(classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "arxiv_df['class'] = arxiv_df['mapped_categories'].apply(lambda x: classes[x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=\"k-means++\", n_clusters=num_classes, n_init=4, random_state=0)\n",
    "estimator = make_pipeline(StandardScaler(), kmeans).fit(df[\"embedding\"].tolist())\n",
    "# estimator = make_pipeline(kmeans).fit(df[\"embedding\"].tolist())\n",
    "\n",
    "df[\"cluster\"] = estimator.predict(df[\"embedding\"].tolist())\n",
    "merged_df = pd.merge(df[['id', 'cluster']], arxiv_df, on=\"id\")\n",
    "\n",
    "accuracy = metrics.accuracy_score(merged_df['class'], merged_df['cluster'])\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
